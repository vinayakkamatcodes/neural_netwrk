{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78c979a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of W1: (10, 64)\n",
      "Shape of b1: (10, 1)\n",
      "Shape of W2: (10, 10)\n",
      "Shape of b2: (10, 1)\n",
      "Shape of W1: (10, 64)\n",
      "Shape of b1: (10, 1)\n",
      "Shape of W2: (10, 10)\n",
      "Shape of b2: (10, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "#load the dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "\n",
    "#Split the dataset \n",
    "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "\n",
    "#Transpose the data matrices to have shape (number of features , number of examples)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "\n",
    "X_train = X_train/16\n",
    "X_test = X_test/16\n",
    "\n",
    "y_train = y_train.reshape(1,-1)\n",
    "y_test = y_test.reshape(1,-1)\n",
    "\n",
    "#Get dimension of data\n",
    "n_x = X_train.shape[0]\n",
    "m = X_train.shape[1]\n",
    "\n",
    "#Define the architecture of the data\n",
    "n_h = 10 #number of neurons in the hidden layer\n",
    "n_y = 10 #number of layers in the output layer(10 digits)\n",
    "\n",
    "def init_params():\n",
    "    \"\"\"\"\n",
    "    Intializes the weights and biases for the neural network\n",
    "\n",
    "    Returns a dictionary containing the inital parameters\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(n_h,n_x)-0.5\n",
    "    b1 = np.random.randn(n_h,1)-0.5\n",
    "\n",
    "    W2 = np.random.randn(n_y,n_h) - 0.5\n",
    "    b2 = np.random.randn(n_y,1)-0.5\n",
    "\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "# calling function to see the initial shapes and values\n",
    "W1,b1,W2,b2 = init_params()\n",
    "\n",
    "print(f\"Shape of W1: {W1.shape}\")\n",
    "print(f\"Shape of b1: {b1.shape}\")\n",
    "print(f\"Shape of W2: {W2.shape}\")\n",
    "print(f\"Shape of b2: {b2.shape}\")\n",
    "\n",
    "# Now, it's your turn.\n",
    "\n",
    "def init_params():\n",
    "    \"\"\"\"\n",
    "    Intializes the weights and biases for the neural network\n",
    "\n",
    "    Returns a dictionary containing the inital parameters\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(n_h,n_x)-0.5\n",
    "    b1 = np.random.randn(n_h,1)-0.5\n",
    "\n",
    "    W2 = np.random.randn(n_y,n_h) - 0.5\n",
    "    b2 = np.random.randn(n_y,1)-0.5\n",
    "\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "# calling function to see the initial shapes and values\n",
    "W1,b1,W2,b2 = init_params()\n",
    "\n",
    "print(f\"Shape of W1: {W1.shape}\")\n",
    "print(f\"Shape of b1: {b1.shape}\")\n",
    "print(f\"Shape of W2: {W2.shape}\")\n",
    "print(f\"Shape of b2: {b2.shape}\")\n",
    "\n",
    "# Now, it's your turn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb17a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Forward Propagation ---\n",
      "Shape of Z1: (10, 1437)\n",
      "Shape of A1: (10, 1437)\n",
      "Shape of Z2: (10, 1437)\n",
      "Shape of A2: (10, 1437)\n",
      "\n",
      "Prediction for first example:\n",
      " [0.09062914 0.19211407 0.05824004 0.27342622 0.15168987 0.04117552\n",
      " 0.02876632 0.04502442 0.10191677 0.01701764]\n",
      "Sum of probabilities for first example: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "def ReLU(Z):\n",
    "    \"\"\"\n",
    "    Implements the Rectified Linear Unit (ReLU) activation function.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- The output of the linear layer, a numpy array of any shape.\n",
    "    \n",
    "    Returns:\n",
    "    A -- The output of ReLU(Z), same shape as Z.\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Implements the Softmax activation function.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- The output of the linear layer, a numpy array of shape (n_y, m).\n",
    "    \n",
    "    Returns:\n",
    "    A -- The output of softmax(Z), a probability distribution over classes.\n",
    "    \"\"\"\n",
    "    # The np.exp(Z - np.max(Z)) is a trick for numerical stability\n",
    "    A = np.exp(Z - np.max(Z)) / sum(np.exp(Z - np.max(Z)))\n",
    "    return A\n",
    "\n",
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for our two-layer network.\n",
    "    \n",
    "    Arguments:\n",
    "    W1, b1, W2, b2 -- The parameters of the model.\n",
    "    X -- The input data of shape (n_x, m).\n",
    "    \n",
    "    Returns:\n",
    "    Z1, A1, Z2, A2 -- Values computed during forward propagation.\n",
    "                       We need these for the backward pass later!\n",
    "    \"\"\"\n",
    "    # Hidden Layer\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "    \n",
    "    # Output Layer\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    \n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "# Let's test it with our initialized parameters and training data\n",
    "# Note: You should have W1, b1, W2, b2 from your init_params() function\n",
    "# and X_train from your data loading step.\n",
    "Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X_train)\n",
    "\n",
    "print(\"\\n--- Forward Propagation ---\")\n",
    "print(f\"Shape of Z1: {Z1.shape}\")\n",
    "print(f\"Shape of A1: {A1.shape}\")\n",
    "print(f\"Shape of Z2: {Z2.shape}\")\n",
    "print(f\"Shape of A2: {A2.shape}\")\n",
    "\n",
    "# A2 contains the predictions for each of the 'm' examples.\n",
    "# Let's look at the prediction for the first example:\n",
    "print(f\"\\nPrediction for first example:\\n {A2[:, 0]}\")\n",
    "print(f\"Sum of probabilities for first example: {np.sum(A2[:, 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b13f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(Y):\n",
    "    \"\"\"\n",
    "    Converts a vector of labels into a one-hot encoded matrix\n",
    "\n",
    "    Arguments:\n",
    "    Y -- The label vector of shape (1, m).\n",
    "\n",
    "    Returns:\n",
    "    one_hot_Y -- A one-hot encoded matrix of shape (n_y, m).\n",
    "    \"\"\"\n",
    "    # Create an m x (max_label + 1) matrix of zeros\n",
    "    one_hot_Y = np.zeros((Y.size, Y.max() + 1)) # <-- FIX IS HERE: added parentheses\n",
    "    \n",
    "    # Set the element at the correct column (label) to 1 for each example\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "\n",
    "    # Transpose to get the correct shape (n_y, m)\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "\n",
    "    return one_hot_Y\n",
    "\n",
    "def backward_prop(Z1,A1,Z2,A2,W2,X,Y):\n",
    "    \"\"\"\"\n",
    "    Implements the backward propogation for the 2 layered network\n",
    "\n",
    "    z1,a1,z2,a2 -- values from the forward propgation\n",
    "    w2 = weight matrix of the output layer\n",
    "    x -- the input data\n",
    "    y -- the true labels\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "\n",
    "    one_hot_y = one_hot(Y)\n",
    "\n",
    "    #output layers of the gradient\n",
    "    dZ2 = A2 - one_hot_y\n",
    "    dW2 = 1/m * dZ2.dot(A1.T)\n",
    "    db2 = 1/m * np.sum(dZ2,axis=1,keepdims=True)\n",
    "\n",
    "    # hidden layer gradients\n",
    "    dZ1 = W2.T.dot(dZ2)*(Z1>0)  # Element wise multiplication for RELU derivative\n",
    "    dW1 = 1/m * dZ1.dot(X.T)\n",
    "    db1 = 1/m * np.sum(dZ1,axis=1,keepdims=True)\n",
    "\n",
    "    return dW1,db1,dW2,db2\n",
    "def update_params(W1,b1,W2,b2,dW1,db1,dW2,db2,alpha):\n",
    "    \n",
    "\n",
    "\n",
    "    W1 = W1 - alpha*dW1\n",
    "    b1 = b1 - alpha*db1\n",
    "    W2 = W2 - alpha*dW2\n",
    "    b2 = b2 - alpha*db2\n",
    "\n",
    "    return W1,b1,W2,b2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b601a599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training ---\n",
      "Iteration: 0\n",
      "[3 3 6 ... 3 3 3] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 16.70%\n",
      "Iteration: 50\n",
      "[6 6 6 ... 6 6 6] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 11.41%\n",
      "Iteration: 100\n",
      "[6 6 6 ... 6 6 6] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 14.96%\n",
      "Iteration: 150\n",
      "[6 6 6 ... 8 6 6] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 19.21%\n",
      "Iteration: 200\n",
      "[6 6 6 ... 8 6 6] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 20.53%\n",
      "Iteration: 250\n",
      "[6 1 6 ... 3 6 6] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 20.88%\n",
      "Iteration: 300\n",
      "[6 1 0 ... 3 6 1] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 22.76%\n",
      "Iteration: 350\n",
      "[6 1 0 ... 3 6 1] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 23.80%\n",
      "Iteration: 400\n",
      "[6 1 0 ... 3 6 1] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 24.57%\n",
      "Iteration: 450\n",
      "[6 1 0 ... 3 6 1] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 26.58%\n",
      "Iteration: 500\n",
      "[6 1 6 ... 3 6 1] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 27.77%\n",
      "Iteration: 550\n",
      "[6 1 6 ... 3 6 1] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 28.60%\n",
      "Iteration: 600\n",
      "[6 1 6 ... 3 6 1] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 29.09%\n",
      "Iteration: 650\n",
      "[6 1 6 ... 3 6 1] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 29.71%\n",
      "Iteration: 700\n",
      "[6 1 6 ... 3 6 6] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 30.13%\n",
      "Iteration: 750\n",
      "[6 1 6 ... 3 6 6] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 30.55%\n",
      "Iteration: 800\n",
      "[6 5 6 ... 3 0 6] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 31.32%\n",
      "Iteration: 850\n",
      "[6 5 6 ... 3 0 6] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 31.52%\n",
      "Iteration: 900\n",
      "[6 5 6 ... 3 0 6] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 32.08%\n",
      "Iteration: 950\n",
      "[6 5 6 ... 3 0 6] [[6 0 0 ... 2 7 1]]\n",
      "Accuracy: 32.50%\n",
      "--- Training Complete ---\n",
      "\n",
      "--- Testing on Test Set ---\n",
      "[[3 3 3 ... 3 3 3]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [3 3 3 ... 3 3 3]\n",
      " ...\n",
      " [0 0 0 ... 3 3 0]\n",
      " [3 3 3 ... 3 3 3]\n",
      " [3 3 3 ... 3 3 3]] [[6 9 3 7 2 1 5 2 5 2 1 9 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6\n",
      "  9 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 5\n",
      "  5 4 7 0 4 5 5 9 9 0 2 3 8 0 6 4 4 9 1 2 8 3 5 2 9 0 4 4 4 3 5 3 1 3 5 9\n",
      "  4 2 7 7 4 4 1 9 2 7 8 7 2 6 9 4 0 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6\n",
      "  9 9 6 9 0 3 5 6 6 0 6 4 3 9 3 9 7 2 9 0 4 5 3 6 5 9 9 8 4 2 1 3 7 7 2 2\n",
      "  3 9 8 0 3 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 8 9 4 8 1 5 4 4 9 6 1 8\n",
      "  6 0 4 5 2 7 4 6 4 5 6 0 3 2 3 6 7 1 5 1 4 7 6 8 8 5 5 1 6 2 8 8 9 9 7 6\n",
      "  2 2 2 3 4 8 8 3 6 0 9 7 7 0 1 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 5 9\n",
      "  9 8 5 3 3 2 0 5 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 1 5 2 1 2 8\n",
      "  7 0 6 4 8 8 5 1 8 4 5 8 7 9 8 5 0 6 2 0 7 9 8 9 5 2 7 7 1 8 7 4 3 8 3 5]]\n",
      "Test Accuracy: 93.61%\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(A2):\n",
    "    \"\"\"\n",
    "    Gets the class predictions from the output probabilities.\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The output probabilities from the softmax layer, shape (n_y, m).\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- A 1D array of predicted class labels.\n",
    "    \"\"\"\n",
    "    return np.argmax(A2, 0) # Returns the index of the max value along axis 0\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the predictions against the true labels.\n",
    "    \n",
    "    Arguments:\n",
    "    predictions -- A 1D array of predicted class labels.\n",
    "    Y -- The true labels, shape (1, m).\n",
    "    \n",
    "    Returns:\n",
    "    accuracy -- The percentage of correct predictions.\n",
    "    \"\"\"\n",
    "    print(predictions, Y)\n",
    "    return np.sum(predictions == Y) / Y.size\n",
    "\n",
    "def gradient_descent(X, Y, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to train the neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- The input training data.\n",
    "    Y -- The true labels for the training data.\n",
    "    alpha -- The learning rate.\n",
    "    iterations -- The number of training iterations (epochs).\n",
    "    \n",
    "    Returns:\n",
    "    W1, b1, W2, b2 -- The trained parameters.\n",
    "    \"\"\"\n",
    "    # 1. Initialize parameters\n",
    "    W1, b1, W2, b2 = init_params() # Use your init_params function\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        # 2. Forward Propagation\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X) # Use your forward_prop\n",
    "        \n",
    "        # 3. Backward Propagation\n",
    "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W2, X, Y) # Use your backward_prop\n",
    "        \n",
    "        # 4. Update Parameters\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha) # Use your update_params\n",
    "        \n",
    "        # 5. Print progress (optional)\n",
    "        if i % 50 == 0: # Print every 50 iterations\n",
    "            print(f\"Iteration: {i}\")\n",
    "            predictions = get_predictions(A2)\n",
    "            print(f\"Accuracy: {get_accuracy(predictions, Y) * 100:.2f}%\")\n",
    "            \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# --- Training the model ---\n",
    "# Make sure X_train and y_train are correctly prepared before this point.\n",
    "# X_train should be transposed and normalized (X_train.T / 16.0)\n",
    "# y_train should be reshaped (y_train.reshape(1, -1))\n",
    "\n",
    "# It's crucial to apply the normalization to X_train and X_test here if you haven't already\n",
    "# For the `load_digits` dataset, pixel values are 0-16, so divide by 16.0\n",
    "# The original prompt suggested 255.0 for a typical 0-255 image. For load_digits, 16.0 is correct.\n",
    "X_train = X_train / 16.0\n",
    "X_test = X_test / 16.0\n",
    "\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "final_W1, final_b1, final_W2, final_b2 = gradient_descent(X_train, y_train, 0.10, 1000) # You can adjust alpha and iterations\n",
    "print(\"--- Training Complete ---\")\n",
    "\n",
    "# Now, test on the test set\n",
    "print(\"\\n--- Testing on Test Set ---\")\n",
    "A2_test = forward_prop(final_W1, final_b1, final_W2, final_b2, X_test)\n",
    "test_predictions = get_predictions(A2_test)\n",
    "test_accuracy = get_accuracy(test_predictions, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b572d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
