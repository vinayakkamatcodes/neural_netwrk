{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f010e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 18:13:14.245081: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-08-19 18:13:14.573213: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-08-19 18:13:16.563626: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 2us/step\n",
      "--- CIFAR-10 Data Initial Shapes ---\n",
      "X_train shape: (50000, 32, 32, 3)\n",
      "y_train shape: (50000, 1)\n",
      "X_test shape: (10000, 32, 32, 3)\n",
      "y_test shape: (10000, 1)\n",
      "\n",
      "--- CIFAR-10 Data After Preprocessing ---\n",
      "X_train shape after normalization: (50000, 32, 32, 3)\n",
      "y_train shape after reshape: (1, 50000)\n",
      "Input image dimensions: 32x32x3\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf # Used only for loading CIFAR-10 dataset\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "# X_train, X_test will be images (num_samples, height, width, channels)\n",
    "# y_train, y_test will be labels (num_samples,)\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(\"--- CIFAR-10 Data Initial Shapes ---\")\n",
    "print(f\"X_train shape: {X_train.shape}\") # (50000, 32, 32, 3) for training images\n",
    "print(f\"y_train shape: {y_train.shape}\") # (50000, 1) for training labels\n",
    "print(f\"X_test shape: {X_test.shape}\")   # (10000, 32, 32, 3) for test images\n",
    "print(f\"y_test shape: {y_test.shape}\")   # (10000, 1) for test labels\n",
    "\n",
    "# --- Preprocessing ---\n",
    "\n",
    "# 1. Normalize pixel values\n",
    "# Convert integers to float and scale to [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# 2. Reshape labels for consistency (optional, but good practice for 1D labels)\n",
    "# y_train and y_test are already (num_samples, 1), so reshape to (1, num_samples) for our previous logic\n",
    "y_train = y_train.reshape(1, -1)\n",
    "y_test = y_test.reshape(1, -1)\n",
    "\n",
    "# 3. Get image dimensions and number of classes\n",
    "input_height, input_width, input_channels = X_train.shape[1:]\n",
    "num_classes = 10 # CIFAR-10 has 10 classes (0-9)\n",
    "\n",
    "print(\"\\n--- CIFAR-10 Data After Preprocessing ---\")\n",
    "print(f\"X_train shape after normalization: {X_train.shape}\")\n",
    "print(f\"y_train shape after reshape: {y_train.shape}\")\n",
    "print(f\"Input image dimensions: {input_height}x{input_width}x{input_channels}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "# We will need these global variables for subsequent layers' initialization\n",
    "# For the CNN, the input 'X' will not be flattened initially.\n",
    "# It will be passed as (num_samples, height, width, channels) or (num_samples, channels, height, width)\n",
    "# We'll stick to (num_samples, height, width, channels) for now and handle channel-first later if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137d2852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Helper Functions for Convolutional Layer ---\n",
    "\n",
    "def zeropad(X, pad):\n",
    "    \"\"\"\n",
    "    Pads the image X with zeros.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- numpy array of shape (m, H, W, C) representing a batch of images\n",
    "    pad -- integer, amount of padding around the borders of an image\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, H + 2*pad, W + 2*pad, C)\n",
    "    \"\"\"\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=0)\n",
    "    return X_pad\n",
    "\n",
    "def initialize_conv_params(filter_size, num_filters, input_channels):\n",
    "    \"\"\"\n",
    "    Initializes filters (weights) and biases for a convolutional layer.\n",
    "    \n",
    "    Arguments:\n",
    "    filter_size -- integer, side length of the square filter (e.g., 3 for 3x3)\n",
    "    num_filters -- integer, number of filters for this layer\n",
    "    input_channels -- integer, number of channels in the input image/feature map\n",
    "    \n",
    "    Returns:\n",
    "    filters -- numpy array of shape (filter_size, filter_size, input_channels, num_filters)\n",
    "               Initialized with small random numbers.\n",
    "    biases -- numpy array of shape (1, 1, 1, num_filters)\n",
    "              Initialized with zeros.\n",
    "    \"\"\"\n",
    "    # Filters are typically initialized with small random values to break symmetry\n",
    "    # and prevent all neurons from learning the same features.\n",
    "    filters = np.random.randn(filter_size, filter_size, input_channels, num_filters) * 0.01\n",
    "    biases = np.zeros((1, 1, 1, num_filters)) # Bias per filter\n",
    "    return filters, biases\n",
    "\n",
    "# --- Convolutional Layer Forward Pass ---\n",
    "\n",
    "def conv2d_forward(X, filters, biases, stride, padding):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolutional layer.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data of shape (m, H_prev, W_prev, C_prev)\n",
    "    filters -- weights of shape (f, f, C_prev, n_C)\n",
    "    biases -- biases of shape (1, 1, 1, n_C)\n",
    "    stride -- integer, specifies the stride length\n",
    "    padding -- string, \"same\" or \"valid\"\n",
    "    \n",
    "    Returns:\n",
    "    Z -- output of the conv layer, numpy array of shape (m, H, W, n_C)\n",
    "    cache -- tuple of values needed for the backward pass: (X, filters, biases, stride, padding)\n",
    "    \"\"\"\n",
    "    m, H_prev, W_prev, C_prev = X.shape\n",
    "    f, f, C_prev, n_C = filters.shape\n",
    "\n",
    "    # Calculate output dimensions\n",
    "    if padding == \"same\":\n",
    "        # Calculate padding amount to maintain same output size for stride 1\n",
    "        pad_h = ((H_prev - 1) * stride + f - H_prev) // 2\n",
    "        pad_w = ((W_prev - 1) * stride + f - W_prev) // 2\n",
    "        X_padded = zeropad(X, pad_h)\n",
    "        H = int((H_prev + 2 * pad_h - f) / stride) + 1\n",
    "        W = int((W_prev + 2 * pad_w - f) / stride) + 1\n",
    "    elif padding == \"valid\":\n",
    "        pad_h, pad_w = 0, 0 # No padding\n",
    "        X_padded = X\n",
    "        H = int((H_prev - f) / stride) + 1\n",
    "        W = int((W_prev - f) / stride) + 1\n",
    "    else:\n",
    "        raise ValueError(\"Padding must be 'same' or 'valid'\")\n",
    "\n",
    "    # Initialize output volume Z\n",
    "    Z = np.zeros((m, H, W, n_C))\n",
    "\n",
    "    # Loop over each example in the batch\n",
    "    for i in range(m):                   # loop over the training examples\n",
    "        x_img = X_padded[i]              # Select ith image from the padded input\n",
    "        # Loop over vertical axis of output volume\n",
    "        for h in range(H):               # loop over vertical axis of the output volume\n",
    "            # Loop over horizontal axis of output volume\n",
    "            for w in range(W):           # loop over horizontal axis of the output volume\n",
    "                # Loop over filters\n",
    "                for c in range(n_C):     # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (patch)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_end + f\n",
    "                    \n",
    "                    # Extract the slice from the image for the current filter\n",
    "                    x_slice = x_img[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # Convolve the slice with the filter and add bias\n",
    "                    # Element-wise product (*) followed by sum for all elements in the slice.\n",
    "                    Z[i, h, w, c] = np.sum(x_slice * filters[:, :, :, c]) + biases[0, 0, 0, c]\n",
    "                                        \n",
    "    cache = (X, filters, biases, stride, padding, X_padded) # Store X_padded for backprop\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "# --- Test the Convolutional Layer Forward Pass ---\n",
    "\n",
    "# Define hyperparameters for our first conv layer\n",
    "conv1_filter_size = 3\n",
    "conv1_num_filters = 16\n",
    "conv1_stride = 1\n",
    "conv1_padding = \"same\"\n",
    "\n",
    "# Initialize parameters for the first conv layer\n",
    "# input_channels for the first layer is 3 (RGB)\n",
    "conv1_filters, conv1_biases = initialize_conv_params(\n",
    "    conv1_filter_size, conv1_num_filters, input_channels\n",
    ")\n",
    "\n",
    "print(f\"\\nShape of Conv1 Filters: {conv1_filters.shape}\") # (f, f, C_prev, n_C) -> (3, 3, 3, 16)\n",
    "print(f\"Shape of Conv1 Biases: {conv1_biases.shape}\")     # (1, 1, 1, n_C) -> (1, 1, 1, 16)\n",
    "\n",
    "# Take a small batch of data to test (e.g., first 5 images)\n",
    "X_batch = X_train[:5]\n",
    "print(f\"Input batch shape for testing: {X_batch.shape}\") # (5, 32, 32, 3)\n",
    "\n",
    "# Perform forward pass\n",
    "Z_conv1, conv1_cache = conv2d_forward(\n",
    "    X_batch, conv1_filters, conv1_biases, conv1_stride, conv1_padding\n",
    ")\n",
    "\n",
    "print(f\"Output shape of Conv1 layer (Z_conv1): {Z_conv1.shape}\")\n",
    "# Expected output: (m, H, W, n_C) -> (5, 32, 32, 16) with \"same\" padding and stride 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa4ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- Pooling Layer Forward Pass ---\n",
    "\n",
    "def max_pool_forward(A_prev, pool_size, stride):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the max pooling layer.\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- input to the pooling layer, numpy array of shape (m, H_prev, W_prev, C_prev)\n",
    "    pool_size -- integer, side length of the square pooling window (e.g., 2 for 2x2)\n",
    "    stride -- integer, specifies the stride length\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, numpy array of shape (m, H, W, C_prev)\n",
    "    cache -- tuple of values needed for the backward pass: (A_prev, pool_size, stride)\n",
    "    \"\"\"\n",
    "    m, H_prev, W_prev, C_prev = A_prev.shape\n",
    "    f = pool_size # Filter size for pooling\n",
    "    \n",
    "    # Calculate output dimensions\n",
    "    H = int((H_prev - f) / stride) + 1\n",
    "    W = int((W_prev - f) / stride) + 1\n",
    "    \n",
    "    # Initialize output volume A\n",
    "    A = np.zeros((m, H, W, C_prev))\n",
    "    \n",
    "    # Loop over each example in the batch\n",
    "    for i in range(m):              # loop over the training examples\n",
    "        a_prev_slice = A_prev[i]    # Select ith image from the input\n",
    "        # Loop over vertical axis of output volume\n",
    "        for h in range(H):          # loop over vertical axis of the output volume\n",
    "            # Loop over horizontal axis of output volume\n",
    "            for w in range(W):      # loop over horizontal axis of the output volume\n",
    "                # Loop over channels (pooling is applied independently to each channel)\n",
    "                for c in range(C_prev): # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (patch)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Extract the slice from the input for the current channel\n",
    "                    a_slice = a_prev_slice[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the max value in the slice and store in output\n",
    "                    A[i, h, w, c] = np.max(a_slice)\n",
    "                                        \n",
    "    cache = (A_prev, pool_size, stride)\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "# --- Test the Pooling Layer Forward Pass ---\n",
    "\n",
    "# Assume Z_conv1 from the previous test is the input to this pooling layer\n",
    "# Use a small batch for testing as done for conv2d_forward\n",
    "X_batch = X_train[:5]\n",
    "\n",
    "# First, run the conv layer to get input for pooling\n",
    "# You'll need to define conv1_filter_size, conv1_num_filters, conv1_stride, conv1_padding, input_channels\n",
    "# from the previous section if you're running this part in isolation.\n",
    "conv1_filter_size = 3\n",
    "conv1_num_filters = 16\n",
    "conv1_stride = 1\n",
    "conv1_padding = \"same\"\n",
    "conv1_filters, conv1_biases = initialize_conv_params(conv1_filter_size, conv1_num_filters, input_channels)\n",
    "Z_conv1, conv1_cache = conv2d_forward(X_batch, conv1_filters, conv1_biases, conv1_stride, conv1_padding)\n",
    "\n",
    "# Now, apply ReLU activation to the convolutional output\n",
    "A_conv1 = np.maximum(0, Z_conv1) # ReLU activation for convolutional layer output\n",
    "\n",
    "print(f\"\\nInput shape to Pooling Layer (A_conv1): {A_conv1.shape}\") # (5, 32, 32, 16)\n",
    "\n",
    "# Define hyperparameters for our first pooling layer\n",
    "pool1_size = 2\n",
    "pool1_stride = 2\n",
    "\n",
    "# Perform forward pass for pooling layer\n",
    "A_pool1, pool1_cache = max_pool_forward(A_conv1, pool1_size, pool1_stride)\n",
    "\n",
    "print(f\"Output shape of Pool1 layer (A_pool1): {A_pool1.shape}\")\n",
    "# Expected output with 32x32 input, 2x2 pool, stride 2: (5, 16, 16, 16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e7894",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Activation Functions (from previous project) ---\n",
    "def ReLU(Z):\n",
    "    \"\"\"\n",
    "    Implements the Rectified Linear Unit (ReLU) activation function.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- The output of the linear layer, a numpy array of any shape.\n",
    "    \n",
    "    Returns:\n",
    "    A -- The output of ReLU(Z), same shape as Z.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Implements the Softmax activation function.\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- The output of the linear layer, a numpy array.\n",
    "    \n",
    "    Returns:\n",
    "    A -- The output of softmax(Z), a probability distribution over classes.\n",
    "    \"\"\"\n",
    "    exp_Z = np.exp(Z - np.max(Z, axis=0, keepdims=True))\n",
    "    A = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    return A\n",
    "\n",
    "# --- Flatten Layer ---\n",
    "def flatten(A):\n",
    "    \"\"\"\n",
    "    Flattens the input array into a 2D array (m, num_features_flattened).\n",
    "    \n",
    "    Arguments:\n",
    "    A -- input array of shape (m, H, W, C)\n",
    "    \n",
    "    Returns:\n",
    "    A_flattened -- flattened array of shape (m, H*W*C)\n",
    "    cache -- the original shape of A for backward pass\n",
    "    \"\"\"\n",
    "    m = A.shape[0]\n",
    "    A_flattened = A.reshape(m, -1) # -1 infers the remaining dimension\n",
    "    cache = A.shape\n",
    "    return A_flattened, cache\n",
    "\n",
    "# --- Dense Layer Initialization ---\n",
    "def initialize_dense_params(input_dim, output_dim):\n",
    "    \"\"\"\n",
    "    Initializes weights and biases for a dense (fully connected) layer.\n",
    "    \n",
    "    Arguments:\n",
    "    input_dim -- dimension of the input to this layer\n",
    "    output_dim -- dimension of the output of this layer\n",
    "    \n",
    "    Returns:\n",
    "    W -- numpy array of shape (output_dim, input_dim)\n",
    "    b -- numpy array of shape (output_dim, 1)\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_dim, input_dim) * 0.01\n",
    "    b = np.zeros((output_dim, 1))\n",
    "    return W, b\n",
    "\n",
    "# --- Dense Layer Forward Pass ---\n",
    "def dense_forward(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a dense layer.\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data), shape (input_dim, m)\n",
    "    W -- weights matrix, shape (output_dim, input_dim)\n",
    "    b -- bias vector, shape (output_dim, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- the linear output, shape (output_dim, m)\n",
    "    A -- the activated output, shape (output_dim, m)\n",
    "    cache -- tuple of values needed for the backward pass: (A_prev, W, b, Z)\n",
    "    \"\"\"\n",
    "    # Adjust A_prev shape if it comes from flatten layer (m, num_features_flattened)\n",
    "    # to (num_features_flattened, m) for matrix multiplication\n",
    "    if A_prev.ndim == 2 and A_prev.shape[0] != W.shape[1]:\n",
    "        A_prev = A_prev.T # Transpose if it's (m, input_dim)\n",
    "\n",
    "    Z = W.dot(A_prev) + b\n",
    "    # A will be calculated by calling activation function separately\n",
    "    \n",
    "    cache = (A_prev, W, b, Z)\n",
    "    return Z, cache\n",
    "\n",
    "# --- Full CNN Forward Propagation ---\n",
    "\n",
    "def cnn_forward_prop(X, params):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for the entire CNN.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data of shape (m, H, W, C)\n",
    "    params -- dictionary containing parameters for all layers\n",
    "    \n",
    "    Returns:\n",
    "    A_L -- the final output probabilities of the network\n",
    "    caches -- list of caches for each layer (conv, pool, flatten, dense)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    \n",
    "    # Layer 1: Conv1 -> ReLU1 -> Pool1\n",
    "    conv1_filters = params['Wc1']\n",
    "    conv1_biases = params['bc1']\n",
    "    A_conv1, conv1_cache = conv2d_forward(X, conv1_filters, conv1_biases, stride=1, padding=\"same\")\n",
    "    A_relu1 = ReLU(A_conv1)\n",
    "    A_pool1, pool1_cache = max_pool_forward(A_relu1, pool_size=2, stride=2)\n",
    "    caches.append((conv1_cache, pool1_cache)) # Store caches for backward pass\n",
    "    \n",
    "    # Layer 2: Conv2 -> ReLU2 -> Pool2\n",
    "    conv2_filters = params['Wc2']\n",
    "    conv2_biases = params['bc2']\n",
    "    A_conv2, conv2_cache = conv2d_forward(A_pool1, conv2_filters, conv2_biases, stride=1, padding=\"same\")\n",
    "    A_relu2 = ReLU(A_conv2)\n",
    "    A_pool2, pool2_cache = max_pool_forward(A_relu2, pool_size=2, stride=2)\n",
    "    caches.append((conv2_cache, pool2_cache))\n",
    "    \n",
    "    # Flatten Layer\n",
    "    A_flat, flatten_cache = flatten(A_pool2)\n",
    "    caches.append(flatten_cache)\n",
    "    \n",
    "    # Dense Layer 1: FC1 -> ReLU3\n",
    "    W_fc1 = params['Wd1']\n",
    "    b_fc1 = params['bd1']\n",
    "    Z_fc1, fc1_cache = dense_forward(A_flat, W_fc1, b_fc1)\n",
    "    A_relu3 = ReLU(Z_fc1)\n",
    "    caches.append(fc1_cache) # Store cache (A_flat, W_fc1, b_fc1, Z_fc1)\n",
    "    \n",
    "    # Dense Layer 2: FC2 -> Softmax (Output Layer)\n",
    "    W_fc2 = params['Wd2']\n",
    "    b_fc2 = params['bd2']\n",
    "    Z_fc2, fc2_cache = dense_forward(A_relu3, W_fc2, b_fc2)\n",
    "    A_softmax = softmax(Z_fc2)\n",
    "    caches.append(fc2_cache) # Store cache (A_relu3, W_fc2, b_fc2, Z_fc2)\n",
    "    \n",
    "    return A_softmax, caches\n",
    "\n",
    "# --- Initialize All Parameters for the CNN Model ---\n",
    "def initialize_cnn_params():\n",
    "    \"\"\"\n",
    "    Initializes all parameters for the entire CNN model.\n",
    "    Architecture:\n",
    "    Conv1 (3x3, 16 filters, stride 1, same padding) -> ReLU -> MaxPool (2x2, stride 2)\n",
    "    Conv2 (3x3, 32 filters, stride 1, same padding) -> ReLU -> MaxPool (2x2, stride 2)\n",
    "    Flatten\n",
    "    Dense1 (128 units) -> ReLU\n",
    "    Dense2 (10 units) -> Softmax (Output)\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing all initialized W and b for conv and dense layers.\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "\n",
    "    # Conv1 Layer\n",
    "    # Input: (H, W, 3) -> (32, 32, 3)\n",
    "    # Output: (32, 32, 16) after conv and relu, (16, 16, 16) after maxpool\n",
    "    params['Wc1'], params['bc1'] = initialize_conv_params(3, 16, input_channels) # 3x3 filter, 16 filters, 3 input channels\n",
    "\n",
    "    # Conv2 Layer\n",
    "    # Input: (16, 16, 16) from Pool1 output\n",
    "    # Output: (16, 16, 32) after conv and relu, (8, 8, 32) after maxpool\n",
    "    params['Wc2'], params['bc2'] = initialize_conv_params(3, 32, 16) # 3x3 filter, 32 filters, 16 input channels from previous layer\n",
    "\n",
    "    # After Conv2 and Pool2: (8, 8, 32)\n",
    "    # Flattened size: 8 * 8 * 32 = 2048\n",
    "    flattened_dim = 8 * 8 * 32 \n",
    "\n",
    "    # Dense Layer 1 (FC1)\n",
    "    # Input: 2048 (from flattened)\n",
    "    # Output: 128 units\n",
    "    params['Wd1'], params['bd1'] = initialize_dense_params(flattened_dim, 128)\n",
    "\n",
    "    # Dense Layer 2 (FC2 - Output Layer)\n",
    "    # Input: 128\n",
    "    # Output: 10 (num_classes)\n",
    "    params['Wd2'], params['bd2'] = initialize_dense_params(128, num_classes)\n",
    "\n",
    "    return params\n",
    "\n",
    "# --- Testing the Full CNN Forward Pass ---\n",
    "\n",
    "# Initialize all parameters for the network\n",
    "cnn_params = initialize_cnn_params()\n",
    "\n",
    "print(\"\\n--- Testing Full CNN Forward Pass ---\")\n",
    "# Take a small batch of data to test (e.g., first 2 images)\n",
    "X_batch_test = X_train[:2]\n",
    "print(f\"Input batch shape for full CNN test: {X_batch_test.shape}\") # (2, 32, 32, 3)\n",
    "\n",
    "# Perform forward pass through the entire CNN\n",
    "final_output_probs, all_caches = cnn_forward_prop(X_batch_test, cnn_params)\n",
    "\n",
    "print(f\"Final output probabilities shape: {final_output_probs.shape}\") # Expected: (num_classes, m) -> (10, 2)\n",
    "print(f\"Sample output probabilities for first image:\\n {final_output_probs[:, 0]}\")\n",
    "print(f\"Sum of probabilities for first image: {np.sum(final_output_probs[:, 0]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c529d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- BACKWARD PASS FUNCTIONS ---\n",
    "\n",
    "def one_hot(Y, num_classes):\n",
    "    \"\"\"\n",
    "    Converts a vector of labels into a one-hot encoded matrix.\n",
    "    \n",
    "    Arguments:\n",
    "    Y -- The label vector of shape (1, m).\n",
    "    num_classes -- The total number of unique classes.\n",
    "    \n",
    "    Returns:\n",
    "    one_hot_Y -- A one-hot encoded matrix of shape (num_classes, m).\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    one_hot_Y = np.zeros((num_classes, m))\n",
    "    one_hot_Y[Y.flatten(), np.arange(m)] = 1\n",
    "    return one_hot_Y\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for a ReLU unit.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    Z -- input of the activation function\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    dZ = np.array(dA, copy=True) # make a copy to avoid modifying original dA\n",
    "    dZ[Z <= 0] = 0 # When Z <= 0, we set dZ to 0 as derivative is 0\n",
    "    return dZ\n",
    "\n",
    "def dense_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for a dense layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output of the current layer (Z)\n",
    "    cache -- tuple of values from forward pass: (A_prev, W, b, Z_linear)\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (output) of the previous layer\n",
    "    dW -- Gradient of the cost with respect to W\n",
    "    db -- Gradient of the cost with respect to b\n",
    "    \"\"\"\n",
    "    A_prev, W, b, Z_linear = cache\n",
    "    m = A_prev.shape[1] if A_prev.ndim == 2 else A_prev.shape[0] # Handle (input_dim, m) or (m, input_dim)\n",
    "\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    # If A_prev was transposed in forward, dA_prev needs to be transposed back\n",
    "    if cache[0].ndim == 2 and cache[0].shape[0] == m and A_prev.shape[0] != W.shape[1]: # This check needs to be more robust\n",
    "         dA_prev = dA_prev.T\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def flatten_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for the flatten layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- Gradient from the next layer (dense), shape (m, H*W*C)\n",
    "    cache -- The original shape of the input to flatten layer (m, H, W, C)\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient reshaped back to original 3D/4D shape\n",
    "    \"\"\"\n",
    "    original_shape = cache\n",
    "    return dA.reshape(original_shape)\n",
    "\n",
    "def create_max_pool_mask(A_prev_slice):\n",
    "    \"\"\"\n",
    "    Creates a mask with 1s at the maximum elements and 0s elsewhere.\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev_slice -- slice of input to max pool layer\n",
    "    \n",
    "    Returns:\n",
    "    mask -- mask matrix\n",
    "    \"\"\"\n",
    "    mask = (A_prev_slice == np.max(A_prev_slice))\n",
    "    return mask\n",
    "\n",
    "def max_pool_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for the max pooling layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of output of max pool layer, shape (m, H, W, C)\n",
    "    cache -- tuple of values from forward pass: (A_prev, pool_size, stride)\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of input to max pool layer, shape (m, H_prev, W_prev, C_prev)\n",
    "    \"\"\"\n",
    "    A_prev, pool_size, stride = cache\n",
    "    m, H_prev, W_prev, C_prev = A_prev.shape\n",
    "    f = pool_size\n",
    "\n",
    "    H, W = dA.shape[1], dA.shape[2] # Output dimensions of pooling layer\n",
    "\n",
    "    dA_prev = np.zeros(A_prev.shape) # Initialize gradient for previous layer\n",
    "\n",
    "    for i in range(m):\n",
    "        a_prev_slice = A_prev[i]\n",
    "        for h in range(H):\n",
    "            for w in range(W):\n",
    "                for c in range(C_prev):\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # Extract the slice from A_prev_slice\n",
    "                    slice_A_prev = a_prev_slice[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Create mask for the max element\n",
    "                    mask = create_max_pool_mask(slice_A_prev)\n",
    "                    \n",
    "                    # Distribute the gradient dA[i, h, w, c] to the max element in dA_prev\n",
    "                    dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += mask * dA[i, h, w, c]\n",
    "                    \n",
    "    return dA_prev\n",
    "\n",
    "def conv2d_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implements the backward pass for a convolutional layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the output of the conv layer, shape (m, H, W, n_C)\n",
    "    cache -- tuple of values from forward pass: (X, filters, biases, stride, padding, X_padded)\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the activation from the previous layer, shape (m, H_prev, W_prev, C_prev)\n",
    "    dW -- gradient of the filters, shape (f, f, C_prev, n_C)\n",
    "    db -- gradient of the biases, shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    X, filters, biases, stride, padding, X_padded = cache\n",
    "    m, H_prev, W_prev, C_prev = X.shape\n",
    "    f, f, C_prev, n_C = filters.shape\n",
    "    m_dz, H_dz, W_dz, n_C_dz = dZ.shape\n",
    "\n",
    "    # Initialize gradients\n",
    "    dA_prev = np.zeros(X.shape)\n",
    "    dW = np.zeros(filters.shape)\n",
    "    db = np.zeros(biases.shape)\n",
    "\n",
    "    # Calculate padding for dA_prev (same as forward pad for same padding)\n",
    "    if padding == \"same\":\n",
    "        pad_h = ((H_prev - 1) * stride + f - H_prev) // 2\n",
    "        pad_w = ((W_prev - 1) * stride + f - W_prev) // 2\n",
    "        dA_prev_padded = zeropad(dA_prev, pad_h)\n",
    "    elif padding == \"valid\":\n",
    "        pad_h, pad_w = 0, 0\n",
    "        dA_prev_padded = dA_prev # No padding means dA_prev_padded is dA_prev\n",
    "    else:\n",
    "        raise ValueError(\"Padding must be 'same' or 'valid'\")\n",
    "\n",
    "    # Loop over each example\n",
    "    for i in range(m):\n",
    "        x_img = X_padded[i] # Padded original input\n",
    "        dz_img = dZ[i]      # Gradient for the current example\n",
    "        da_prev_padded_img = dA_prev_padded[i] # Padded gradient for previous layer\n",
    "\n",
    "        for h in range(H_dz):\n",
    "            for w in range(W_dz):\n",
    "                for c in range(n_C): # Loop over output channels\n",
    "                    # Find the corners of the current \"slice\" (patch)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "\n",
    "                    # Slice from X_padded (for dW)\n",
    "                    x_slice = x_img[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Slice from da_prev_padded_img (for dA_prev)\n",
    "                    # We need to add the gradient from this convolution output\n",
    "                    # to the corresponding part of dA_prev_padded_img\n",
    "                    \n",
    "                    # Update dW and db\n",
    "                    dW[:, :, :, c] += x_slice * dz_img[h, w, c]\n",
    "                    db[:, :, :, c] += dz_img[h, w, c]\n",
    "\n",
    "                    # Update dA_prev_padded_img\n",
    "                    # The filter is conceptually \"flipped\" or \"rotated\" for dA_prev calculation\n",
    "                    # (effectively, this is a full convolution/transposed convolution)\n",
    "                    # Here, we're performing a form of 'transposed convolution' or 'full convolution'\n",
    "                    # which involves adding the dZ value * filter value to dA_prev\n",
    "                    da_prev_padded_img[vert_start:vert_end, horiz_start:horiz_end, :] += \\\n",
    "                        filters[:, :, :, c] * dz_img[h, w, c]\n",
    "        \n",
    "        # After processing all h, w, c for one example, unpad dA_prev_padded_img\n",
    "        if padding == \"same\":\n",
    "            dA_prev[i, :, :, :] = da_prev_padded_img[pad_h:da_prev_padded_img.shape[0]-pad_h,\n",
    "                                                     pad_w:da_prev_padded_img.shape[1]-pad_w, :]\n",
    "        elif padding == \"valid\":\n",
    "            dA_prev[i, :, :, :] = da_prev_padded_img\n",
    "                    \n",
    "    # Average over batch size\n",
    "    dW = dW / m\n",
    "    db = db / m\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "# --- Full CNN Backward Propagation ---\n",
    "\n",
    "def cnn_backward_prop(A_softmax, Y, caches, params):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation for the entire CNN.\n",
    "    \n",
    "    Arguments:\n",
    "    A_softmax -- output probabilities of the network from forward pass\n",
    "    Y -- true labels (1, m)\n",
    "    caches -- list of caches from forward pass\n",
    "    params -- dictionary of parameters\n",
    "    \n",
    "    Returns:\n",
    "    grads -- dictionary containing gradients for all parameters (dW, db for all layers)\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    m = Y.shape[1] # Number of examples in the batch\n",
    "    num_classes = A_softmax.shape[0]\n",
    "\n",
    "    # Convert Y to one-hot encoding for loss calculation\n",
    "    one_hot_Y = one_hot(Y, num_classes)\n",
    "\n",
    "    # Start backward pass from the last layer (Dense2)\n",
    "    \n",
    "    # Dense Layer 2 (Output Layer): Softmax + Cross-Entropy Loss\n",
    "    # dZ for softmax + cross-entropy is A_softmax - one_hot_Y\n",
    "    dZ_fc2 = A_softmax - one_hot_Y\n",
    "    \n",
    "    fc2_cache = caches.pop() # Retrieve cache for FC2\n",
    "    dA_relu3, dWd2, dbd2 = dense_backward(dZ_fc2, fc2_cache)\n",
    "    grads['dWd2'] = dWd2\n",
    "    grads['dbd2'] = dbd2\n",
    "\n",
    "    # Dense Layer 1: ReLU3 -> FC1 (Backward through ReLU then FC1)\n",
    "    fc1_cache, A_relu3 = caches.pop() # Retrieve cache for FC1 and A_relu3\n",
    "    dZ_fc1 = relu_backward(dA_relu3, fc1_cache[3]) # fc1_cache[3] is Z_fc1 from forward pass\n",
    "    dA_flat, dWd1, dbd1 = dense_backward(dZ_fc1, fc1_cache)\n",
    "    grads['dWd1'] = dWd1\n",
    "    grads['dbd1'] = dbd1\n",
    "\n",
    "    # Flatten Layer (Backward)\n",
    "    flatten_cache = caches.pop() # Retrieve flatten cache (original_shape)\n",
    "    dA_pool2 = flatten_backward(dA_flat, flatten_cache)\n",
    "\n",
    "    # Layer 2: Pool2 -> ReLU2 -> Conv2 (Backward through Pool2 then ReLU2 then Conv2)\n",
    "    conv2_cache, pool2_cache, A_relu2 = caches.pop() # Retrieve caches for Conv2, Pool2, and A_relu2\n",
    "    dA_relu2 = max_pool_backward(dA_pool2, pool2_cache)\n",
    "    dZ_conv2 = relu_backward(dA_relu2, conv2_cache[0]) # conv2_cache[0] is original X (input to conv2)\n",
    "                                                      # Correction: Z from conv2 (Z_conv2) for relu_backward\n",
    "    # For ReLU backward, we need the Z that went *into* ReLU. A_relu2 came from ReLU(Z_conv2).\n",
    "    # So, we need Z_conv2 from `conv2_cache` (which actually stores X, filters, biases, stride, padding, X_padded)\n",
    "    # The actual Z_conv2 itself needs to be passed or re-calculated if not in cache.\n",
    "    # We should have stored Z_conv2 from `cnn_forward_prop` for ReLU backward.\n",
    "    # Let's adjust `cnn_forward_prop` to store Z values for ReLU.\n",
    "    # For now, let's assume A_relu2 is dA, and Z_conv2 is Z for relu_backward.\n",
    "    # The cache should store Z_conv for ReLU backward.\n",
    "    # The problem is that in `cnn_forward_prop`, we store `A_relu1` and `A_relu2` but not `Z_conv1` and `Z_conv2`\n",
    "    # which are needed for `relu_backward`.\n",
    "    \n",
    "    # Corrected cache storage in `cnn_forward_prop` to include Z_convX for ReLU backward:\n",
    "    # caches.append((conv1_cache, pool1_cache, Z_conv1))\n",
    "    # caches.append((conv2_cache, pool2_cache, Z_conv2))\n",
    "    # Let's assume Z_conv2 is available in `conv2_cache` as `Z_conv2` (it's not currently, only `X_padded`)\n",
    "    # To fix this, we need to modify `cnn_forward_prop` to store Z_conv1 and Z_conv2.\n",
    "    # For now, let's pass `None` for Z and make relu_backward handle it gracefully (it won't).\n",
    "    # This means the current `relu_backward` call will fail without the correct Z value.\n",
    "    # Let's adjust the `cnn_forward_prop` in this code block to pass Z_conv.\n",
    "\n",
    "    # Re-reading `cnn_forward_prop` cache storage:\n",
    "    # caches.append((conv1_cache, pool1_cache, A_relu1)) # <-- This stores A_relu1, not Z_conv1\n",
    "    # caches.append((conv2_cache, pool2_cache, A_relu2)) # <-- This stores A_relu2, not Z_conv2\n",
    "\n",
    "    # Let's fix cnn_forward_prop to store Z_conv for ReLU backward\n",
    "    # We will pass Z_conv1 for A_relu1 in first tuple\n",
    "    # We will pass Z_conv2 for A_relu2 in second tuple\n",
    "    # And Z_fc1 for A_relu3 in third tuple\n",
    "    \n",
    "    # --- TEMPORARY FIX FOR TESTING (will be properly fixed later in cnn_forward_prop) ---\n",
    "    # For `relu_backward(dA, Z)`, Z needs to be the `Z` that went *into* ReLU.\n",
    "    # This `Z` comes from the output of the linear part of the previous layer.\n",
    "    # For A_relu2, the `Z` was `Z_conv2`.\n",
    "    # For A_relu3, the `Z` was `Z_fc1`.\n",
    "    \n",
    "    # We need the Z_conv2 for this dZ_conv2 calculation. It's not in cache explicitly.\n",
    "    # It means we need to get Z_conv2 from the conv2_cache.\n",
    "    # However, conv2_cache is (X, filters, biases, stride, padding, X_padded) -- it does not contain Z_conv2.\n",
    "    # This implies that `cnn_forward_prop`'s caching strategy needs to be modified\n",
    "    # to store the Z value for each activation layer.\n",
    "    # Let's update cnn_forward_prop to store Z values directly.\n",
    "    # This is a major change, so I'll put it in the next Canvas.\n",
    "    # For now, let's assume the Z values are retrievable.\n",
    "\n",
    "    # FIXING THE CACHE STORAGE IN `cnn_forward_prop` for the provided code block\n",
    "    # (This means the current cnn_forward_prop in the Canvas needs to be updated first)\n",
    "    # The `cnn_forward_prop` should return:\n",
    "    # caches.append((conv1_cache, pool1_cache, Z_conv1)) # Store Z_conv1\n",
    "    # caches.append((conv2_cache, pool2_cache, Z_conv2)) # Store Z_conv2\n",
    "    # caches.append((fc1_cache, Z_fc1)) # Store Z_fc1\n",
    "\n",
    "    # Assuming `Z_conv2` is available from somewhere, for now, let's use `A_relu2` as a placeholder\n",
    "    # which will likely lead to incorrect gradients for ReLU backward, but allows the structure to run.\n",
    "    # The correct implementation needs the Z before ReLU for `relu_backward`.\n",
    "    \n",
    "    # Let's stick to the current cache structure and fix it properly in the next step.\n",
    "    # For relu_backward, we need the Z values that were passed to ReLU.\n",
    "    # In cnn_forward_prop, we store A_relu1, A_relu2, A_relu3. These are the *outputs* of ReLU.\n",
    "    # We need the *inputs* to ReLU (Z_conv1, Z_conv2, Z_fc1).\n",
    "    # I will modify cnn_forward_prop to store these Z values.\n",
    "\n",
    "    # Let's assume Z_conv2 is the linear output from the conv2_forward call\n",
    "    # This requires accessing `Z_conv2` directly, which is not in `conv2_cache` (X, filters, biases, stride, padding, X_padded)\n",
    "    # The `cnn_forward_prop` function needs to change how it stores caches.\n",
    "    # I will modify the provided cnn_forward_prop to store `Z_conv` values for ReLU backward.\n",
    "\n",
    "    # --- REVISED cnn_forward_prop (to be included in the next Canvas with backprop) ---\n",
    "    # def cnn_forward_prop(X, params):\n",
    "    #     caches = []\n",
    "    #     # Layer 1: Conv1 -> ReLU1 -> Pool1\n",
    "    #     A_conv1, conv1_cache = conv2d_forward(X, params['Wc1'], params['bc1'], stride=1, padding=\"same\")\n",
    "    #     Z_relu1 = A_conv1 # Store Z for ReLU\n",
    "    #     A_relu1 = ReLU(Z_relu1)\n",
    "    #     A_pool1, pool1_cache = max_pool_forward(A_relu1, pool_size=2, stride=2)\n",
    "    #     caches.append(((conv1_cache, Z_relu1), pool1_cache)) # Store Z_relu1 for ReLU backward\n",
    "        \n",
    "    #     # Layer 2: Conv2 -> ReLU2 -> Pool2\n",
    "    #     A_conv2, conv2_cache = conv2d_forward(A_pool1, params['Wc2'], params['bc2'], stride=1, padding=\"same\")\n",
    "    #     Z_relu2 = A_conv2 # Store Z for ReLU\n",
    "    #     A_relu2 = ReLU(Z_relu2)\n",
    "    #     A_pool2, pool2_cache = max_pool_forward(A_relu2, pool_size=2, stride=2)\n",
    "    #     caches.append(((conv2_cache, Z_relu2), pool2_cache)) # Store Z_relu2 for ReLU backward\n",
    "        \n",
    "    #     # Flatten Layer\n",
    "    #     A_flat, flatten_cache = flatten(A_pool2)\n",
    "    #     caches.append(flatten_cache)\n",
    "        \n",
    "    #     # Dense Layer 1: FC1 -> ReLU3\n",
    "    #     Z_fc1_linear, fc1_cache = dense_forward(A_flat, params['Wd1'], params['bd1'])\n",
    "    #     Z_relu3 = Z_fc1_linear # Store Z for ReLU\n",
    "    #     A_relu3 = ReLU(Z_relu3)\n",
    "    #     caches.append((fc1_cache, Z_relu3)) # Store Z_relu3 for ReLU backward\n",
    "        \n",
    "    #     # Dense Layer 2: FC2 -> Softmax (Output Layer)\n",
    "    #     Z_fc2_linear, fc2_cache = dense_forward(A_relu3, params['Wd2'], params['bd2'])\n",
    "    #     A_softmax = softmax(Z_fc2_linear)\n",
    "    #     caches.append(fc2_cache)\n",
    "        \n",
    "    #     return A_softmax, caches\n",
    "    # --- END REVISED cnn_forward_prop ---\n",
    "\n",
    "    # For now, let's assume `Z_conv2_from_cache` is passed from a modified `cnn_forward_prop`\n",
    "    # or that Z_conv is the first element of conv_cache. It is not, this will cause an error\n",
    "    # if `relu_backward` expects `Z` value from the `Z_conv`\n",
    "    \n",
    "    # We need to make sure that the cache for ReLU backward contains the Z value.\n",
    "    # The current cache from cnn_forward_prop for conv layers is (conv_cache, pool_cache, A_relu).\n",
    "    # It should be (conv_cache, pool_cache, Z_relu_input).\n",
    "    \n",
    "    # I will modify `cnn_forward_prop` immediately below this to properly store Z values.\n",
    "    # Then the backprop will work.\n",
    "\n",
    "    # Assume the `cnn_forward_prop` *has been modified* to store the Z values.\n",
    "    # (conv_cache, pool_cache, Z_relu_input)\n",
    "    conv2_sub_cache, pool2_cache, Z_relu2_input = caches.pop() # Z_relu2_input is Z_conv2\n",
    "    dA_relu2_prev = max_pool_backward(dA_pool2, pool2_cache)\n",
    "    dZ_conv2 = relu_backward(dA_relu2_prev, Z_relu2_input)\n",
    "    dA_pool1_prev, dWc2, dbc2 = conv2d_backward(dZ_conv2, conv2_sub_cache)\n",
    "    grads['dWc2'] = dWc2\n",
    "    grads['dbc2'] = dbc2\n",
    "\n",
    "    # Layer 1: Pool1 -> ReLU1 -> Conv1 (Backward through Pool1 then ReLU1 then Conv1)\n",
    "    conv1_sub_cache, pool1_cache, Z_relu1_input = caches.pop() # Z_relu1_input is Z_conv1\n",
    "    dA_relu1_prev = max_pool_backward(dA_pool1_prev, pool1_cache)\n",
    "    dZ_conv1 = relu_backward(dA_relu1_prev, Z_relu1_input)\n",
    "    dA_prev_final, dWc1, dbc1 = conv2d_backward(dZ_conv1, conv1_sub_cache)\n",
    "    grads['dWc1'] = dWc1\n",
    "    grads['dbc1'] = dbc1\n",
    "\n",
    "    return grads\n",
    "\n",
    "# --- Update Parameters ---\n",
    "def update_cnn_params(params, grads, alpha):\n",
    "    \"\"\"\n",
    "    Updates the parameters of the CNN using gradient descent.\n",
    "    \n",
    "    Arguments:\n",
    "    params -- dictionary containing current parameters\n",
    "    grads -- dictionary containing gradients\n",
    "    alpha -- learning rate\n",
    "    \n",
    "    Returns:\n",
    "    params -- updated dictionary of parameters\n",
    "    \"\"\"\n",
    "    # Update Dense Layer 2\n",
    "    params['Wd2'] -= alpha * grads['dWd2']\n",
    "    params['bd2'] -= alpha * grads['dbd2']\n",
    "\n",
    "    # Update Dense Layer 1\n",
    "    params['Wd1'] -= alpha * grads['dWd1']\n",
    "    params['bd1'] -= alpha * grads['dbd1']\n",
    "\n",
    "    # Update Conv Layer 2\n",
    "    params['Wc2'] -= alpha * grads['dWc2']\n",
    "    params['bc2'] -= alpha * grads['dbc2']\n",
    "\n",
    "    # Update Conv Layer 1\n",
    "    params['Wc1'] -= alpha * grads['dWc1']\n",
    "    params['bc1'] -= alpha * grads['dbc1']\n",
    "\n",
    "    return params\n",
    "\n",
    "# --- Full CNN Forward Propagation (MODIFIED TO STORE Z FOR ReLU BACKWARD) ---\n",
    "# This modification is crucial for the backward pass to work correctly.\n",
    "# The previous `cnn_forward_prop` stored A_relu, but relu_backward needs Z_relu_input.\n",
    "def cnn_forward_prop(X, params):\n",
    "    caches = []\n",
    "    \n",
    "    # Layer 1: Conv1 -> ReLU1 -> Pool1\n",
    "    A_conv1, conv1_cache = conv2d_forward(X, params['Wc1'], params['bc1'], stride=1, padding=\"same\")\n",
    "    Z_relu1_input = A_conv1 # Store the input to ReLU for backward pass\n",
    "    A_relu1 = ReLU(Z_relu1_input)\n",
    "    A_pool1, pool1_cache = max_pool_forward(A_relu1, pool_size=2, stride=2)\n",
    "    caches.append((conv1_cache, pool1_cache, Z_relu1_input)) # Now storing Z_relu1_input\n",
    "    \n",
    "    # Layer 2: Conv2 -> ReLU2 -> Pool2\n",
    "    A_conv2, conv2_cache = conv2d_forward(A_pool1, params['Wc2'], params['bc2'], stride=1, padding=\"same\")\n",
    "    Z_relu2_input = A_conv2 # Store the input to ReLU for backward pass\n",
    "    A_relu2 = ReLU(Z_relu2_input)\n",
    "    A_pool2, pool2_cache = max_pool_forward(A_relu2, pool_size=2, stride=2)\n",
    "    caches.append((conv2_cache, pool2_cache, Z_relu2_input)) # Now storing Z_relu2_input\n",
    "    \n",
    "    # Flatten Layer\n",
    "    A_flat, flatten_cache = flatten(A_pool2)\n",
    "    caches.append(flatten_cache)\n",
    "    \n",
    "    # Dense Layer 1: FC1 -> ReLU3\n",
    "    Z_fc1_linear, fc1_cache = dense_forward(A_flat, params['Wd1'], params['bd1'])\n",
    "    Z_relu3_input = Z_fc1_linear # Store the input to ReLU for backward pass\n",
    "    A_relu3 = ReLU(Z_relu3_input)\n",
    "    caches.append((fc1_cache, Z_relu3_input)) # Now storing Z_relu3_input\n",
    "    \n",
    "    # Dense Layer 2: FC2 -> Softmax (Output Layer)\n",
    "    Z_fc2_linear, fc2_cache = dense_forward(A_relu3, params['Wd2'], params['bd2'])\n",
    "    A_softmax = softmax(Z_fc2_linear)\n",
    "    caches.append(fc2_cache) # Still storing (A_relu3, W_fc2, b_fc2, Z_fc2_linear)\n",
    "    \n",
    "    return A_softmax, caches\n",
    "\n",
    "# --- Prediction and Accuracy Functions (from previous project, adapted for CNN output) ---\n",
    "def get_cnn_predictions(A_softmax):\n",
    "    \"\"\"\n",
    "    Gets the class predictions from the output probabilities.\n",
    "    \n",
    "    Arguments:\n",
    "    A_softmax -- The output probabilities from the softmax layer, shape (num_classes, m).\n",
    "    \n",
    "    Returns:\n",
    "    predictions -- A 1D array of predicted class labels.\n",
    "    \"\"\"\n",
    "    return np.argmax(A_softmax, axis=0)\n",
    "\n",
    "def get_cnn_accuracy(predictions, Y):\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the predictions against the true labels.\n",
    "    \n",
    "    Arguments:\n",
    "    predictions -- A 1D array of predicted class labels.\n",
    "    Y -- The true labels, shape (1, m).\n",
    "    \n",
    "    Returns:\n",
    "    accuracy -- The percentage of correct predictions.\n",
    "    \"\"\"\n",
    "    # Y is (1, m), predictions is (m,). Flatten Y for comparison.\n",
    "    return np.sum(predictions == Y.flatten()) / Y.size\n",
    "\n",
    "# --- Gradient Descent (Training Loop) for CNN ---\n",
    "def cnn_gradient_descent(X, Y, alpha, iterations):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to train the CNN.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- The input training data (m, H, W, C).\n",
    "    Y -- The true labels for the training data (1, m).\n",
    "    alpha -- The learning rate.\n",
    "    iterations -- The number of training iterations (epochs).\n",
    "    \n",
    "    Returns:\n",
    "    params -- The trained parameters.\n",
    "    \"\"\"\n",
    "    params = initialize_cnn_params() # Initialize all CNN parameters\n",
    "    \n",
    "    print(\"\\n--- Starting CNN Training ---\")\n",
    "    for i in range(1, iterations + 1):\n",
    "        # Forward Propagation\n",
    "        A_softmax, caches = cnn_forward_prop(X, params)\n",
    "        \n",
    "        # Backward Propagation\n",
    "        grads = cnn_backward_prop(A_softmax, Y, caches, params)\n",
    "        \n",
    "        # Update Parameters\n",
    "        params = update_cnn_params(params, grads, alpha)\n",
    "        \n",
    "        # Print progress\n",
    "        if i % 5 == 0 or i == 1: # Print every 5 iterations or at the first\n",
    "            predictions = get_cnn_predictions(A_softmax)\n",
    "            current_accuracy = get_cnn_accuracy(predictions, Y) * 100\n",
    "            print(f\"Iteration: {i}, Training Accuracy: {current_accuracy:.2f}%\")\n",
    "            \n",
    "    print(\"--- CNN Training Complete ---\")\n",
    "    return params\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure X_train and y_train are correctly prepared (already done above)\n",
    "\n",
    "    # Train the model\n",
    "    learning_rate = 0.001 # CNNs often use smaller learning rates\n",
    "    num_iterations = 20 # Start with a small number of iterations for testing backprop\n",
    "                        # Full training will require many more.\n",
    "\n",
    "    print(f\"\\nTraining CNN with Learning Rate: {learning_rate}, Iterations: {num_iterations}\")\n",
    "    trained_cnn_params = cnn_gradient_descent(X_train[:500], y_train[:, :500], learning_rate, num_iterations)\n",
    "    # Using a small subset of data (e.g., first 500 images) for faster testing of backprop.\n",
    "    # For full training, use X_train, y_train.\n",
    "\n",
    "    # Test the model on the unseen test set\n",
    "    print(\"\\n--- Testing on Test Set ---\")\n",
    "    # Perform forward propagation on the test data using the trained parameters\n",
    "    A_softmax_test, _ = cnn_forward_prop(X_test, trained_cnn_params)\n",
    "    \n",
    "    # Get predictions and calculate accuracy on the test set\n",
    "    test_predictions = get_cnn_predictions(A_softmax_test)\n",
    "    test_accuracy = get_cnn_accuracy(test_predictions, y_test) * 100\n",
    "    print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    # Optional: Display a sample prediction (requires matplotlib)\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # index = 0 # Change this to view different examples\n",
    "    # sample_image = X_test[index]\n",
    "    # predicted_class = test_predictions[index]\n",
    "    # actual_class = y_test[0, index]\n",
    "\n",
    "    # plt.imshow(sample_image)\n",
    "    # plt.title(f\"Predicted: {predicted_class}, Actual: {actual_class}\")\n",
    "    # plt.axis('off')\n",
    "    # plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
